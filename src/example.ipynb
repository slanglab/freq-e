{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freq-E example usage \n",
    "Starting with a collection of text documents. This notebook walks through: \n",
    "\n",
    "1. Pre-processing text to be in a format suitable for input into freq-e\n",
    "\n",
    "2. How to run freq-e to obtain prevalence estimates on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np \n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import estimate; reload(estimate)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "We will use the Yelp academic dataset as an example. The text representation will be unigram counts (e.g. \"bag-of-words\") which have already been processed and loaded. \n",
    "\n",
    "X : numpy.ndarray\n",
    "\n",
    "y : numpy.ndarray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_y_from_json(file_name): \n",
    "    count_dicts = []\n",
    "    y = []\n",
    "    for line in open(file_name): \n",
    "        dd = json.loads(line)\n",
    "        counts = dd['counts'].copy()\n",
    "        cc = dd['class']\n",
    "        count_dicts.append(counts); y.append(cc)\n",
    "    return count_dicts, np.array(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocab(X_train, dv_vocab): \n",
    "    #remove words that occur in <5 docs \n",
    "    xx=X_train.copy()\n",
    "    xx[xx>0]=1\n",
    "    w_df = np.asarray(xx.sum(0)).flatten()\n",
    "    new_vocab_mask = w_df >= 5\n",
    "    print(\"Orig vocab %d, pruned %d\" % (len(w_df), np.sum(new_vocab_mask)))\n",
    "    X_train = X_train[:,new_vocab_mask]\n",
    "    dv_vocab = dv_vocab[new_vocab_mask]\n",
    "    return X_train, dv_vocab, new_vocab_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig vocab 14791, pruned 3112\n",
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
      "(2000, 3112) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#get train data \n",
    "dv = DictVectorizer()\n",
    "train_count_dicts, y_train = load_x_y_from_json('example_data/train_yelp.json')\n",
    "X_train = dv.fit_transform(train_count_dicts).toarray()\n",
    "dv_vocab = np.array(dv.feature_names_)\n",
    "X_train, dv_vocab, new_vocab_mask = prune_vocab(X_train, dv_vocab)\n",
    "print(type(X_train), type(y_train))\n",
    "print(X_train.shape, y_train.shape)\n",
    "assert X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test(test_count_dicts, new_vocab_mask): \n",
    "    X_test = dv.transform(test_count_dicts).toarray()\n",
    "    X_test = X_test[:,new_vocab_mask]\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
      "(2000, 3112) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# get test data (1 test group) \n",
    "# NOTE: the test group is the \"inference\" group in a real-word setting\n",
    "# here we have labels on the test set, but in a real-word setting there \n",
    "# would most likely not be labels on the test set\n",
    "test_count_dicts, y_test = load_x_y_from_json('example_data/test_yelp.json')\n",
    "X_test = transform_test(test_count_dicts, new_vocab_mask)\n",
    "print(type(X_test), type(y_test))\n",
    "print(X_test.shape, y_test.shape)\n",
    "assert X_test.shape[1] == X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freq-e usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "- In order to select the best discriminitive classifier, we will do a grid search over the L1 penalties for LogReg, evaluating on cross-entropy over 10 cross-validation folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DISCRIMINATIVE MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Training mean accuracy= 0.9635\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "freq_e = estimate.FreqEstimate(conf_level=0.95)\n",
    "trained_model = freq_e.fit(X_train, y_train)\n",
    "print(trained_model.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Returns (1) a point estimate of the class frequency/proportions and (2) a confidence interval for the point estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.769\n"
     ]
    }
   ],
   "source": [
    "# here we can know the true proportions because we have access to the test labels \n",
    "print('TRUE')\n",
    "print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.769417372043\n"
     ]
    }
   ],
   "source": [
    "#naive method = PCC (probabilistic classify and count)\n",
    "print('PCC')\n",
    "print(np.mean(trained_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'conf_interval': (0.748, 0.79500000000000004), 'point': 0.77200000000000002}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "out = freq_e.predict_freq(trained_model, y_train, X_test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test group high prevalence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 3112) (415,)\n"
     ]
    }
   ],
   "source": [
    "test_count_dicts2, y_test2 = load_x_y_from_json('testgroup_zpoZ6WyQUYff18-z4ZU1mA/zpoZ6WyQUYff18-z4ZU1mA')\n",
    "X_test2 = transform_test(test_count_dicts2, new_vocab_mask)\n",
    "print(X_test2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.968674698795\n"
     ]
    }
   ],
   "source": [
    "print('TRUE')\n",
    "print(np.mean(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.901939188497\n"
     ]
    }
   ],
   "source": [
    "print('PCC')\n",
    "print(np.mean(trained_model.predict_proba(X_test2)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'conf_interval': (0.94400000000000006, 0.99199999999999999), 'point': 0.97299999999999998}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "out = freq_e.predict_freq(trained_model, y_train, X_test2)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test group with low prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825, 3112) (825,)\n"
     ]
    }
   ],
   "source": [
    "test_count_dicts3, y_test3 = load_x_y_from_json('dWFUKB_HPBIE87AFBHEb_w')\n",
    "X_test3 = transform_test(test_count_dicts3, new_vocab_mask)\n",
    "print(X_test3.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.13696969697\n"
     ]
    }
   ],
   "source": [
    "print('TRUE')\n",
    "print(np.mean(y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.311479369453\n"
     ]
    }
   ],
   "source": [
    "print('PCC')\n",
    "print(np.mean(trained_model.predict_proba(X_test3)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'conf_interval': (0.028000000000000001, 0.069000000000000006), 'point': 0.045999999999999999}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "out = freq_e.predict_freq(trained_model, y_train, X_test3)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
