{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freq-E tutorial \n",
    "This notebook walks through: \n",
    "\n",
    "1. Getting text in a format suitable for input into freq-e\n",
    "\n",
    "2. How to run freq-e to obtain prevalence estimates on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dependencies \n",
    "import numpy as np \n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load freq_e package \n",
    "from freq_e import estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Pre-processing \n",
    "The goal of pre-processing is to get your data into the format: \n",
    "- `X_train` : numpy.ndarray, shape=(number of training examples, number of features) \n",
    "- `y_train` : numpy.ndarray (binary 0's and 1's), shape=(number of training examples,) \n",
    "- `X_test` : numpy.ndarray, shape=(number of test/inference examples, number of *training* features)\n",
    "\n",
    "Often you will have *multiple* test groups. In this case you should have an `X_test` for each test group. \n",
    "\n",
    "We will use the Yelp academic dataset as an example. The text representation will be unigram counts (e.g. \"bag-of-words\"). Here, we have already calcuated the BOW counts and saved them as a .json file. The y-values are negative sentiment (y=0) and positive sentiment (y=1). \n",
    "\n",
    "You can skip this section if you already have your data formatted. Common other feature representations include averaging word embeddings or getting a FastText vector representation of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_y_from_json(file_name): \n",
    "    count_dicts = []\n",
    "    y = []\n",
    "    for line in open(file_name): \n",
    "        dd = json.loads(line)\n",
    "        counts = dd['counts'].copy()\n",
    "        cc = dd['class']\n",
    "        count_dicts.append(counts); y.append(cc)\n",
    "    return count_dicts, np.array(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocab(X_train, dv_vocab): \n",
    "    #remove words that occur in <5 docs \n",
    "    xx=X_train.copy()\n",
    "    xx[xx>0]=1\n",
    "    w_df = np.asarray(xx.sum(0)).flatten()\n",
    "    new_vocab_mask = w_df >= 5\n",
    "    print(\"Orig vocab %d, pruned %d\" % (len(w_df), np.sum(new_vocab_mask)))\n",
    "    X_train = X_train[:,new_vocab_mask]\n",
    "    dv_vocab = dv_vocab[new_vocab_mask]\n",
    "    return X_train, dv_vocab, new_vocab_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig vocab 14791, pruned 3112\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2000, 3112) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#get train data \n",
    "dv = DictVectorizer()\n",
    "train_count_dicts, y_train = load_x_y_from_json('example_data/train_yelp.json')\n",
    "X_train = dv.fit_transform(train_count_dicts).toarray()\n",
    "dv_vocab = np.array(dv.feature_names_)\n",
    "X_train, dv_vocab, new_vocab_mask = prune_vocab(X_train, dv_vocab)\n",
    "print(type(X_train), type(y_train))\n",
    "print(X_train.shape, y_train.shape)\n",
    "assert X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test(test_count_dicts, new_vocab_mask): \n",
    "    X_test = dv.transform(test_count_dicts).toarray()\n",
    "    X_test = X_test[:,new_vocab_mask]\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2000, 3112) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# get test data (1 test group) \n",
    "# NOTE: the test group is the \"inference\" group in a real-word setting\n",
    "# here we have labels on the test set, but in a real-word setting there \n",
    "# would most likely not be labels on the test set\n",
    "test_count_dicts, y_test = load_x_y_from_json('example_data/test_yelp.json')\n",
    "X_test = transform_test(test_count_dicts, new_vocab_mask)\n",
    "print(type(X_test), type(y_test))\n",
    "print(X_test.shape, y_test.shape)\n",
    "assert X_test.shape[1] == X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Freq-e usage\n",
    "\n",
    "## Inference\n",
    "Freq-e inference will return (1) a point estimate of the class frequency/proportions and (2) a confidence interval for the point estimate. \n",
    "\n",
    "There are three different ways to obtain estimates: \n",
    "1. Create a `FreqEstimate` object and use the built-in training method. \n",
    "2. Use the `infer_freq()` method and pass in a pre-trained scikit-learn linear model (e.g. Logistic_Regression). Here the model class is restricted to scikit-learn models that have a .decision_function() method. \n",
    "3. Use the `infer_freq()` method and pass in the predicted probabilities of the positive class of the test set. \n",
    "\n",
    "Method 3 may be useful in the cases where you have special classifier architectures that are not built from sklearn (e.g. an LSTM or CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 (train internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a freq-e object \n",
    "FreqE = estimate.FreqEstimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "In order to select the best discriminitive classifier, we do a grid search over the L1 penalties for LogReg, evaluating on cross-entropy over 10 cross-validation folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LOGISTIC REGRESSION MODEL\n",
      "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Training mean accuracy= 0.9635\n"
     ]
    }
   ],
   "source": [
    "FreqE.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'point': 0.772, 'conf_interval': (0.748, 0.795)}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "out = FreqE.infer_freq_obj(X_test, conf_level=0.95)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare this to other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.769\n"
     ]
    }
   ],
   "source": [
    "# In our example, we know the true class proportion because we have access to the test labels\n",
    "# This is not the case if you are doing true inference \n",
    "print('TRUE')\n",
    "print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.7694195295507668\n"
     ]
    }
   ],
   "source": [
    "#naive method = PCC (probabilistic classify and count)\n",
    "print('PCC')\n",
    "print(np.mean(FreqE.trained_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 (pre-trained scikit-learn linear model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will give an example of passing in a classifier that is not LogisticRegression \n",
    "# We will show that we can also use a LinearSVC (linear support vector classifier)\n",
    "from sklearn.svm import LinearSVC\n",
    "trained_model = LinearSVC()\n",
    "trained_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'point': 0.5740000000000001, 'conf_interval': (0.538, 0.609)}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "label_prior = np.mean(y_train) #most often, you want to estimate the label prior from training, but you can also pass in other values\n",
    "out = estimate.infer_freq(X_test, label_prior, conf_level=0.95, trained_model=trained_model)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a lot worse since we obviously haven't tuned the hyperparameters for the SVC. However, if a user is using a classifier that has been fine-tuned on the training data (e.g. a LSTM or Transformer) this method should work well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 (pass in predicted probabilites on the test set)\n",
    "Train a classifier, get the predicted positive class probabilites on the test set (`test_pred_probs`) and pass this into the `infer_freq` stand-alone method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.75550754e-01 5.67237177e-05 9.99967122e-01 9.80429407e-01\n",
      " 7.78366952e-01]\n"
     ]
    }
   ],
   "source": [
    "# Let's train a LogisticRegression classifier (without hyperparameter tuning) \n",
    "# and get the probabilities for the positive class \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "trained_model2 = LogisticRegression()\n",
    "trained_model2.fit(X_train, y_train)\n",
    "test_pred_probs = trained_model2.predict_proba(X_test)[:, 1]\n",
    "print(test_pred_probs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'point': 0.789, 'conf_interval': (0.767, 0.81)}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "label_prior = np.mean(y_train)\n",
    "out = estimate.infer_freq(X_test, label_prior, conf_level=0.95, trained_model=None, test_pred_probs=test_pred_probs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other test groups \n",
    "Here we examine two other test groups to show some anecdotes (supported by empirical results in Keith et. al 2018) of why using our method is important when the training class prevalence does not match the true test class prevalence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High prevalence test group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 3112) (415,)\n"
     ]
    }
   ],
   "source": [
    "test_count_dicts2, y_test2 = load_x_y_from_json('example_data/high_prev.json')\n",
    "X_test2 = transform_test(test_count_dicts2, new_vocab_mask)\n",
    "print(X_test2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.9686746987951808\n"
     ]
    }
   ],
   "source": [
    "print('TRUE')\n",
    "print(np.mean(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'point': 0.973, 'conf_interval': (0.9440000000000001, 0.992)}\n"
     ]
    }
   ],
   "source": [
    "# Note: we don't have to re-train our frequency estimate object! we can just infer the \n",
    "# class proportions on this new test set \n",
    "print('FREQ-E ESTIMATED')\n",
    "out = FreqE.infer_freq_obj(X_test2, conf_level=0.95)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.9019396973852466\n"
     ]
    }
   ],
   "source": [
    "print('PCC')\n",
    "print(np.mean(FreqE.trained_model.predict_proba(X_test2)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low prevalence test group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825, 3112) (825,)\n"
     ]
    }
   ],
   "source": [
    "test_count_dicts3, y_test3 = load_x_y_from_json('example_data/low_prev.json')\n",
    "X_test3 = transform_test(test_count_dicts3, new_vocab_mask)\n",
    "print(X_test3.shape, y_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE\n",
      "0.13696969696969696\n"
     ]
    }
   ],
   "source": [
    "print('TRUE')\n",
    "print(np.mean(y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ-E ESTIMATED\n",
      "{'point': 0.046, 'conf_interval': (0.028, 0.069)}\n"
     ]
    }
   ],
   "source": [
    "print('FREQ-E ESTIMATED')\n",
    "out = FreqE.infer_freq_obj(X_test3, conf_level=0.95)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC\n",
      "0.3114853708952336\n"
     ]
    }
   ],
   "source": [
    "print('PCC')\n",
    "print(np.mean(FreqE.trained_model.predict_proba(X_test3)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
